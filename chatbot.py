# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11lSoOYDKe2NJrklrZsdjhWBoECZmJXm6
"""

import nltk
nltk.download('punkt')
from nltk.stem.porter import PorterStemmer
import json
import numpy as np
#pip install tflearn
import tflearn
import random
import pickle

"""Tokenization"""

def tokenize(sentence):
    return nltk.word_tokenize(sentence)

"""Lowering & Stemming"""

stemmer = PorterStemmer()
def stem(word):
    return stemmer.stem(word.lower())

"""Bag of Words"""

def bag_of_words(tokenized_sentence, all_words):
    tokenized_sentence = [stem(w) for w in tokenized_sentence ]
    bag = np.zeros(len(all_words), dtype= np.float32)
    for idx, w in enumerate(all_words):
        if w in tokenized_sentence:
            bag[idx] = 1.0
    return bag

"""Reading Tags, patterns from json file & applying tokenization ans stemming"""

with open('intents.json','r') as f:
    intents = json.load(f)
    
all_words = []
tags =[]
xy = []
for intent in intents['intents']:
    tag = intent['tag']
    tags.append(tag)
    for pattern in intent['patterns']:
        w = tokenize(pattern)
        all_words.extend(w)
        xy.append((w,tag))
ignore_words = ['!','?','.',',']
all_words = [stem(w) for w in all_words if w not in ignore_words]
all_words = sorted(set(all_words))    # Set removes duplicate
tags = sorted(tags)

"""Creating training data using Bag of Words"""

X_train = []
y_train = []
for (pattern_sentence, tag) in xy:
    bag = bag_of_words(pattern_sentence,all_words)
    X_train.append(bag)
    output_words = [0]*len(tags)
    output_words = list(output_words)
    output_words[tags.index(tag)] = 1
    y_train.append(output_words)
    
X_train = np.array(X_train)
y_train = np.array(y_train)

"""Modeling"""

# reset underlying graph data
#tf.reset_default_graph()
# Build neural network
ip_layer = tflearn.input_data(shape=[None, len(X_train[0])])
hidden1 = tflearn.fully_connected(ip_layer, 8)
hidden2 = tflearn.fully_connected(hidden1, 8)
op_layer = tflearn.fully_connected(hidden2, len(y_train[0]), activation='softmax')
net = tflearn.regression(op_layer)

# Define model and setup tensorboard
model = tflearn.DNN(net, tensorboard_dir='tflearn_logs')
# Start training (apply gradient descent algorithm)
model.fit(X_train, y_train, n_epoch=1000, batch_size=8, show_metric=True)
print('Final Accuracy:',model.evaluate(X_train,y_train))

"""Creating BOW for User Inputs"""

def user_input(sentence):
  # Tokenization
  sentence = tokenize(sentence)
  # lowering and stemming
  # Generating Bag of Words
  bow = bag_of_words(sentence, all_words)
  return np.array(bow)

"""Response Model"""

THRESHOLD=0.25
def response(sentence):
  result = user_input(sentence)
  results = model.predict([result])[0]
  # filter out predictions below a threshold
  results = [[i,r] for i,r in enumerate(results) if r>THRESHOLD]
  # sort by strength of probability
  results.sort(key=lambda x: x[1], reverse=True)
  return_list = []
  for r in results:
      return_list.append((tags[r[0]], r[1]))
    # return tuple of intent and probability
  return return_list

def chatbot(sentence):
  result = response(sentence)
  if result:
    while result:

      for i in intents['intents']:
      # find a tag matching the first result
        if i['tag'] == result[0][0]:
        # a random response from the intent
          return print(random.choice(i['responses']))
  else:
    print('Please enter a Valid Response')

"""Saving all the files"""

model.save('model.tflearn')
pickle.dump( {'tags':tags, 'all_words':all_words, 'X_train':X_train, 'y_train':y_train}, open( "training_data", "wb" ) )

from flask import Flask, render_template, request
app = Flask(__name__)
app.static_folder = 'static'
@app.route("/")
def home():
    return render_template("index.html")
@app.route("/get")
def get_bot_response():
    userText = request.args.get('msg')
    return chatbot(userText)
if __name__ == "__main__":
    app.run()