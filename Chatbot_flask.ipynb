{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f38e0910",
   "metadata": {},
   "source": [
    "# Chatbot Flask Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44883848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import flask\n",
    "from flask import Flask, render_template, request\n",
    "from tensorflow.keras.models import load_model\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import nltk\n",
    "import pickle\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbc6fe3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-05 20:58:01.263829: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-05-05 20:58:01.264329: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 8. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    }
   ],
   "source": [
    "app = Flask(__name__)\n",
    "\n",
    "# Reading Saved Files\n",
    "model = load_model('model.h5')\n",
    "intents = json.loads(open('intents.json').read())\n",
    "all_words = pickle.load(open('all_words.pkl','rb'))\n",
    "tags = pickle.load(open('tags.pkl','rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6b08b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sentence):\n",
    "    return nltk.word_tokenize(sentence)\n",
    "stemmer = PorterStemmer()\n",
    "def stem(word):\n",
    "    return stemmer.stem(word.lower())\n",
    "def bag_of_words(tokenized_sentence, all_words):\n",
    "    tokenized_sentence = [stem(w) for w in tokenized_sentence ]\n",
    "    bag = np.zeros(len(all_words), dtype= np.float32)\n",
    "    for idx, w in enumerate(all_words):\n",
    "        if w in tokenized_sentence:\n",
    "            bag[idx] = 1.0\n",
    "    return bag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c021023",
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_input(sentence):    \n",
    "    # Tokenization\n",
    "    sentence = tokenize(sentence)\n",
    "    # lowering and stemming\n",
    "    # Generating Bag of Words\n",
    "    bow = bag_of_words(sentence, all_words)\n",
    "    return np.array(bow)\n",
    "\n",
    "THRESHOLD=0.25\n",
    "def response(sentence):\n",
    "    p = user_input(sentence)\n",
    "    results = model.predict(np.array([p]))[0]\n",
    "  # filter out predictions below a threshold\n",
    "    results = [[i,r] for i,r in enumerate(results) if r>THRESHOLD]\n",
    "  # sort by strength of probability\n",
    "    results.sort(key=lambda x: x[1], reverse=True)\n",
    "    return_list = []\n",
    "    for r in results:\n",
    "        return_list.append((tags[r[0]], r[1]))\n",
    "    # return tuple of intent and probability\n",
    "    return return_list\n",
    "\n",
    "def chatbot(sentence):\n",
    "    result = response(sentence)\n",
    "    if result:\n",
    "        while result:\n",
    "\n",
    "            for i in intents['intents']:\n",
    "      # find a tag matching the first result\n",
    "                if i['tag'] == result[0][0]:\n",
    "        # a random response from the intent\n",
    "                    return (random.choice(i['responses']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dce3776c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__' (lazy loading)\n",
      " * Environment: production\n",
      "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
      "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n",
      "127.0.0.1 - - [05/May/2022 20:58:11] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [05/May/2022 20:58:11] \"GET /static/styles/style.css HTTP/1.1\" 304 -\n",
      "127.0.0.1 - - [05/May/2022 20:58:14] \"GET /get?msg=Hi HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [05/May/2022 20:58:22] \"GET /get?msg=what%20time%20are%20you%20open HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [05/May/2022 20:58:27] \"GET /get?msg=what%20do%20you%20sell HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [05/May/2022 20:58:46] \"GET /get?msg=what%20payment%20methods%20do%20you%20accept HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [05/May/2022 20:58:54] \"GET /get?msg=thankyou HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [05/May/2022 20:58:59] \"GET /get?msg=thank%20you HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [05/May/2022 20:59:04] \"GET /get?msg=ty HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "app.static_folder = 'static'\n",
    "@app.route(\"/\")\n",
    "def home():\n",
    "    return render_template(\"index.html\")\n",
    "@app.route(\"/get\")\n",
    "def get_bot_response():\n",
    "    userText = request.args.get('msg')\n",
    "    return chatbot(userText)\n",
    "if __name__ == \"__main__\":\n",
    "    app.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
